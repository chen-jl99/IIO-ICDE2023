{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7082785e-cff5-4ef2-80ce-8e9fa3a3bcc9",
   "metadata": {
    "executionInfo": {
     "elapsed": 528,
     "status": "ok",
     "timestamp": 1653877674146,
     "user": {
      "displayName": "Junlin Chen",
      "userId": "04548097444605488042"
     },
     "user_tz": -480
    },
    "id": "7082785e-cff5-4ef2-80ce-8e9fa3a3bcc9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import csv\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "\n",
    "from params import *\n",
    "from functions import *\n",
    "\n",
    "import os\n",
    "os.makedirs('results/', exist_ok = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dd5e48-246e-4e7f-a1a9-e1bc8541c06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for SEED in range(0,10):\n",
    "\n",
    "    set_seed(SEED)\n",
    "    data_hist = pd.read_pickle(f'data/data_hist_{SEED}.pickle')\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    kmeans = KMeans(n_clusters = K, random_state = SEED)\n",
    "\n",
    "    pred_prob_original = np.zeros((N_hist,len(passenger_incentive_list),len(driver_incentive_list)))\n",
    "    pred_prob_adjusted = np.zeros((N_hist,len(passenger_incentive_list),len(driver_incentive_list)))\n",
    "    \n",
    "    fare_list = np.zeros(N_hist)\n",
    "    compensation_list = np.zeros(N_hist)\n",
    "    feature_list = np.zeros((N_hist, num_feature))\n",
    "    \n",
    "    error_list = np.zeros(N_hist)\n",
    "\n",
    "    for n in range(0,N_hist):\n",
    "\n",
    "        # obtain the feature for each data\n",
    "        fare_list[n] = data_hist.loc[n, 'fare']\n",
    "        compensation_list[n] = data_hist.loc[n, 'compensation']\n",
    "        feature_list[n,:] = data_hist.loc[n, feature_name].values\n",
    "\n",
    "        for i in range(0, len(passenger_incentive_list)):\n",
    "            for j in range(0, len(driver_incentive_list)):\n",
    "                pred_prob_original[n,i,j] = data_hist.loc[n, f'pred_prob_{i}_{j}']\n",
    "                \n",
    "        i = np.argwhere(passenger_incentive_list == data_hist.loc[n, 'passenger_incentive'])[0][0]\n",
    "        j = np.argwhere(driver_incentive_list == data_hist.loc[n, 'driver_incentive'])[0][0]\n",
    "        \n",
    "        # obtain the error for each data\n",
    "        error_list[n] = data_hist.loc[n, f'true_label_{i}_{j}'] - data_hist.loc[n, f'pred_prob_{i}_{j}']\n",
    "        \n",
    "    # normalize and cluster the features of all historical data\n",
    "    cluster_idx = kmeans.fit_predict(scaler.fit_transform(feature_list))\n",
    "    feature_cluster_list = kmeans.cluster_centers_\n",
    "    error_cluster_list = np.zeros(K)\n",
    "    size_cluster_list = np.zeros(K)\n",
    "\n",
    "    for k in range(0, K):\n",
    "        # calculate the average error information for each cluster\n",
    "        error_cluster_list[k] = np.mean(error_list[cluster_idx == k])\n",
    "        # calculate the size information for each cluster\n",
    "        size_cluster_list[k] = np.sum(cluster_idx == k)\n",
    "        \n",
    "    for n in range(0,N_hist):\n",
    "        # adjust the probability using the clustering information\n",
    "        kernel = pairwise_kernels(feature_cluster_list, scaler.transform(feature_list[n].reshape(1, -1)), metric='rbf', gamma=gamma).flatten()\n",
    "        error = np.sum(kernel*size_cluster_list*error_cluster_list)/np.sum(kernel*size_cluster_list)          \n",
    "        pred_prob_adjusted[n] = pred_prob_original[n] + error\n",
    "    \n",
    "    lambda_h = solve_largrange_multiplier(N_hist, pred_prob_adjusted, fare_list, compensation_list, passenger_incentive_list, driver_incentive_list, B=B_hist, r=0)\n",
    "    print('Seed', SEED, 'Multiplier', lambda_h)\n",
    "    \n",
    "    pickle.dump([lambda_h, scaler, feature_cluster_list, error_cluster_list, size_cluster_list], open(f'results/results_offline_SHR_CLUSTER_{SEED}.pickle', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "All_demo_direct_0530_deg.ipynb",
   "provenance": [
    {
     "file_id": "1m9oOZ2Mf3R2AwJrsCPG_PpvGkhuDdBhP",
     "timestamp": 1653115203881
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
